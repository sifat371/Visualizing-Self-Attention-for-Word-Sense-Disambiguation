{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRyTgkTsHy_U",
        "outputId": "3eeaa78f-d132-4f72-aeb9-e9189ed33d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected dataset size: 26 sentences\n"
          ]
        }
      ],
      "source": [
        "# Our two \"meanings\" (classes)\n",
        "LABEL_MAP = {\n",
        "    \"Financial\": 0,\n",
        "    \"River\": 1\n",
        "}\n",
        "\n",
        "data = [\n",
        "    (\"I went to the bank to deposit money\", 4, LABEL_MAP[\"Financial\"]),\n",
        "    (\"He sat on the river bank\", 5, LABEL_MAP[\"River\"]),\n",
        "    (\"The savings bank is closed today\", 2, LABEL_MAP[\"Financial\"]),\n",
        "    (\"The boat is near the bank\", 5, LABEL_MAP[\"River\"]),\n",
        "    (\"I need a loan from the bank\", 6, LABEL_MAP[\"Financial\"]),\n",
        "    (\"The river bank was muddy\", 2, LABEL_MAP[\"River\"]),\n",
        "\n",
        "    # --- \"Rule Breaker\" Sentences ---\n",
        "    (\"The river bank is high today\", 2, LABEL_MAP[\"River\"]),\n",
        "    (\"The bank is open for business\", 1, LABEL_MAP[\"Financial\"]),\n",
        "    (\"This bank is my favorite for loans\", 1, LABEL_MAP[\"Financial\"]),\n",
        "    (\"We went to the river bank\", 5, LABEL_MAP[\"River\"]),\n",
        "\n",
        "    # --- More \"Financial\" Examples ---\n",
        "    (\"He opened a new bank account\", 3, LABEL_MAP[\"Financial\"]),\n",
        "    (\"I need to transfer money at the bank\", 7, LABEL_MAP[\"Financial\"]),\n",
        "    (\"The bank approved my mortgage\", 1, LABEL_MAP[\"Financial\"]),\n",
        "    (\"My paycheck goes to the bank\", 5, LABEL_MAP[\"Financial\"]),\n",
        "    (\"The bank charged an overdraft fee\", 1, LABEL_MAP[\"Financial\"]),\n",
        "    (\"Online bank security is important\", 1, LABEL_MAP[\"Financial\"]),\n",
        "    (\"The ATM at the bank is broken\", 4, LABEL_MAP[\"Financial\"]),\n",
        "    (\"He works at the bank\", 4, LABEL_MAP[\"Financial\"]),\n",
        "\n",
        "    # --- More \"River\" Examples ---\n",
        "    (\"We had a picnic on the river bank\", 6, LABEL_MAP[\"River\"]),\n",
        "    (\"The fishing boat passed the bank\", 5, LABEL_MAP[\"River\"]),\n",
        "    (\"Erosion is affecting the bank\", 4, LABEL_MAP[\"River\"]),\n",
        "    (\"We saw ducks on the bank\", 4, LABEL_MAP[\"River\"]),\n",
        "    (\"The bank was steep and grassy\", 1, LABEL_MAP[\"River\"]),\n",
        "    (\"Flood waters rose over the bank\", 5, LABEL_MAP[\"River\"]),\n",
        "    (\"The north bank of the river is flooded\", 2, LABEL_MAP[\"River\"]),\n",
        "    (\"Trees line the bank of the stream\", 3, LABEL_MAP[\"River\"]),\n",
        "]\n",
        "\n",
        "print(f\"Corrected dataset size: {len(data)} sentences\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Collect all words\n",
        "all_words = set()\n",
        "for sentence, index, label in data:\n",
        "    words_in_sentence = sentence.lower().split() # split() turns the string into a list of words\n",
        "    all_words.update(words_in_sentence)         # .update() adds all items from the list to the set\n",
        "\n",
        "# 2. Create the vocabulary mapping\n",
        "# Start with 0 for the PAD token\n",
        "vocab = {\"<PAD>\": 0}\n",
        "index = 1\n",
        "for word in sorted(list(all_words)): # sort them so the order is consistent\n",
        "    vocab[word] = index\n",
        "    index += 1\n",
        "\n",
        "print(\"\\nYour vocabulary:\")\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKCas0oQfiez",
        "outputId": "27c94967-6094-4aa1-a997-7651f37fbebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Your vocabulary:\n",
            "{'<PAD>': 0, 'a': 1, 'account': 2, 'affecting': 3, 'an': 4, 'and': 5, 'approved': 6, 'at': 7, 'atm': 8, 'bank': 9, 'boat': 10, 'broken': 11, 'business': 12, 'charged': 13, 'closed': 14, 'deposit': 15, 'ducks': 16, 'erosion': 17, 'favorite': 18, 'fee': 19, 'fishing': 20, 'flood': 21, 'flooded': 22, 'for': 23, 'from': 24, 'goes': 25, 'grassy': 26, 'had': 27, 'he': 28, 'high': 29, 'i': 30, 'important': 31, 'is': 32, 'line': 33, 'loan': 34, 'loans': 35, 'money': 36, 'mortgage': 37, 'muddy': 38, 'my': 39, 'near': 40, 'need': 41, 'new': 42, 'north': 43, 'of': 44, 'on': 45, 'online': 46, 'open': 47, 'opened': 48, 'over': 49, 'overdraft': 50, 'passed': 51, 'paycheck': 52, 'picnic': 53, 'river': 54, 'rose': 55, 'sat': 56, 'savings': 57, 'saw': 58, 'security': 59, 'steep': 60, 'stream': 61, 'the': 62, 'this': 63, 'to': 64, 'today': 65, 'transfer': 66, 'trees': 67, 'was': 68, 'waters': 69, 'we': 70, 'went': 71, 'works': 72}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim  # You'll need this for the optimizer\n",
        "import math"
      ],
      "metadata": {
        "id": "a1cMVDebjFlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Input Layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # 2. Self-Attention Layers\n",
        "        self.q_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.k_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.v_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        # 3. Output Layer\n",
        "        self.output_layer = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, sentence_indices, target_word_index):\n",
        "        # sentence_indices is a list of word IDs, e.g., [1, 2, 3, 4, 5, 6]\n",
        "\n",
        "        # --- 1. Get Initial Embeddings ---\n",
        "        X = self.embedding(sentence_indices) # Shape: [seq_len, embedding_dim]\n",
        "\n",
        "        # --- 2. Implement Self-Attention ---\n",
        "        Q = self.q_layer(X)\n",
        "        K = self.k_layer(X)\n",
        "        V = self.v_layer(X)\n",
        "\n",
        "        Scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "\n",
        "        scale_factor = math.sqrt(self.embedding_dim)\n",
        "        Scaled_Scores = Scores / scale_factor\n",
        "\n",
        "        # We save these weights for our visualization!\n",
        "        # Shape: [seq_len, seq_len]\n",
        "        self.Attention_Weights = F.softmax(Scaled_Scores, dim=-1)\n",
        "\n",
        "        Output = torch.matmul(self.Attention_Weights, V)\n",
        "\n",
        "        # --- 3. Get the Final Classification ---\n",
        "\n",
        "        # We only care about the vector for our *target word*\n",
        "        # Shape: [embedding_dim]\n",
        "        target_word_vector = Output[target_word_index]\n",
        "\n",
        "        # Pass it to the output layer to get the final class scores\n",
        "        # Shape: [num_classes]\n",
        "        final_scores = self.output_layer(target_word_vector)\n",
        "\n",
        "        return final_scores, self.Attention_Weights"
      ],
      "metadata": {
        "id": "X1EqzgD5no51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cb68b08"
      },
      "source": [
        "### Explanation of the Attention Mechanism in `SimpleAttentionModel`\n",
        "\n",
        "The `SimpleAttentionModel` uses a simplified form of self-attention to determine the meaning of the target word (\"bank\" in most cases) based on the other words in the sentence. Here's how it works within the `forward` method:\n",
        "\n",
        "1.  **Input Embeddings**:\n",
        "    *   The input `sentence_indices` is a tensor of integer IDs representing the words in the sentence.\n",
        "    *   `self.embedding(sentence_indices)` converts these IDs into dense numerical vectors. This `X` tensor has the shape `[seq_len, embedding_dim]`, where `seq_len` is the number of words in the sentence and `embedding_dim` is the size of each word vector.\n",
        "\n",
        "2.  **Query, Key, and Value Projections**:\n",
        "    *   The `X` tensor is passed through three separate linear layers: `self.q_layer`, `self.k_layer`, and `self.v_layer`.\n",
        "    *   These layers transform the input embeddings into three new tensors: `Q` (Query), `K` (Key), and `V` (Value). Each also has the shape `[seq_len, embedding_dim]`.\n",
        "    *   Intuitively, the Query vector for a word represents \"what I'm looking for\", the Key vector represents \"what I contain\", and the Value vector represents \"what information I provide\".\n",
        "\n",
        "3.  **Calculating Attention Scores**:\n",
        "    *   `Scores = torch.matmul(Q, K.transpose(-2, -1))` calculates the raw attention scores. This is a matrix multiplication between the Query matrix `Q` and the transpose of the Key matrix `K`.\n",
        "    *   The resulting `Scores` tensor has the shape `[seq_len, seq_len]`. Each element `Scores[i, j]` represents the dot product between the Query vector of word `i` and the Key vector of word `j`. A higher dot product means word `i` finds word `j` more relevant.\n",
        "\n",
        "4.  **Scaling**:\n",
        "    *   `scale_factor = math.sqrt(self.embedding_dim)` is calculated. Scaling the scores by the square root of the embedding dimension is a common technique to prevent the dot products from becoming too large, which could lead to vanishing gradients during training.\n",
        "    *   `Scaled_Scores = Scores / scale_factor` applies this scaling.\n",
        "\n",
        "5.  **Calculating Attention Weights**:\n",
        "    *   `self.Attention_Weights = F.softmax(Scaled_Scores, dim=-1)` applies the softmax function along the last dimension (`dim=-1`).\n",
        "    *   Softmax converts the scaled scores into probability-like weights that sum up to 1 for each row. The `Attention_Weights` tensor, also `[seq_len, seq_len]`, now indicates how much each word *attends* to every other word in the sentence. `Attention_Weights[i, j]` is the weight word `i` places on word `j`.\n",
        "    *   These weights are stored in `self.Attention_Weights` so they can be accessed and visualized later.\n",
        "\n",
        "6.  **Calculating the Output (Weighted Sum of Values)**:\n",
        "    *   `Output = torch.matmul(self.Attention_Weights, V)` performs another matrix multiplication. This time, the `Attention_Weights` matrix is multiplied by the Value matrix `V`.\n",
        "    *   The resulting `Output` tensor has the shape `[seq_len, embedding_dim]`. The vector for each word `i` in this `Output` tensor is a weighted sum of all the Value vectors in the sentence, where the weights are determined by how much word `i` attended to each word (the `i`-th row of `Attention_Weights`).\n",
        "\n",
        "7.  **Focusing on the Target Word**:\n",
        "    *   `target_word_vector = Output[target_word_index]` extracts the vector from the `Output` tensor that corresponds to the `target_word_index`. This vector now contains information about the target word, influenced by the context of the entire sentence through the attention mechanism.\n",
        "\n",
        "8.  **Final Classification**:\n",
        "    *   `final_scores = self.output_layer(target_word_vector)` passes the target word's context-aware vector through a final linear layer.\n",
        "    *   This layer produces the final scores (`[num_classes]`) for each possible class (Financial or River), which are then used to make the final prediction.\n",
        "\n",
        "In essence, the attention mechanism allows the model to dynamically weigh the importance of different words in the sentence when trying to understand the meaning of a specific target word, making it particularly useful for tasks like word sense disambiguation. The visualization in the later cell (`PY6IyGENoKC5`) shows exactly these calculated attention weights for the target word."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_tensor(sentence, vocab_map):\n",
        "    \"\"\"Converts a string sentence to a tensor of integer IDs.\"\"\"\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    # Look up each word in the vocab, use 0 (<PAD>) if it's not found\n",
        "    # (Though for this tiny dataset, all words will be found)\n",
        "    indices = [vocab_map.get(word, 0) for word in words]\n",
        "\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Test it on one sentence:\n",
        "test_sentence = data[0][0] # \"I went to the bank to deposit money\"\n",
        "test_tensor = sentence_to_tensor(test_sentence, vocab)\n",
        "print(f\"Sentence: {test_sentence}\")\n",
        "print(f\"Tensor:   {test_tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo-FlQiQn3Gv",
        "outputId": "1bec7fa0-b915-42b1-fb6a-dd7e74d4b65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: I went to the bank to deposit money\n",
            "Tensor:   tensor([30, 71, 64, 62,  9, 64, 15, 36])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "# These are the \"settings\" for our model\n",
        "VOCAB_SIZE = len(vocab)            # We get this from the vocab we just built\n",
        "EMBEDDING_DIM = 32                 # You can pick any size. 32 is small and fast.\n",
        "NUM_CLASSES = len(LABEL_MAP)       # This is 2 (Financial or River)\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 200                       # How many times to loop over the data\n",
        "\n",
        "# Initialization\n",
        "\n",
        "# 1. Create the model\n",
        "model = SimpleAttentionModel(VOCAB_SIZE, EMBEDDING_DIM, NUM_CLASSES)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. Create the \"mechanic\" (updates the model to make it better)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Model, Loss, and Optimizer are ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOtrnH4coBd5",
        "outputId": "910bfe72-5670-4e82-8887-ee44b0858de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model, Loss, and Optimizer are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting Training ---\")\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    # Loop over every sentence in our dataset\n",
        "    for sentence, target_index, label in data:\n",
        "\n",
        "        # 1. Clear old gradients\n",
        "        # PyTorch adds up gradients, so we reset them to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Prepare inputs\n",
        "        # Convert sentence to tensor of IDs\n",
        "        input_indices = sentence_to_tensor(sentence, vocab)\n",
        "        # Convert label to a tensor\n",
        "        target_label = torch.tensor([label], dtype=torch.long)\n",
        "\n",
        "        # 3. Forward pass (Get the model's prediction)\n",
        "        # We run the model's \"forward\" function\n",
        "        scores, _ = model(input_indices, target_index)\n",
        "\n",
        "        # 4. Calculate loss (How wrong was the prediction?)\n",
        "        # CrossEntropyLoss expects scores as [Batch, Classes] and labels as [Batch]\n",
        "        # Our scores are just [Classes], so we .unsqueeze(0) to add a batch dim of 1\n",
        "        loss = loss_function(scores.unsqueeze(0), target_label)\n",
        "\n",
        "        # 5. Backward pass and optimize\n",
        "        loss.backward()  # Calculate all the gradients\n",
        "        optimizer.step() # Update the model's weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print the average loss every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss / len(data):.4f}\")\n",
        "\n",
        "print(\"--- Training Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W89kSQPdoF6w",
        "outputId": "62878959-ab8a-4660-8163-d8e952f79912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Training ---\n",
            "Epoch 20/200, Loss: 0.0057\n",
            "Epoch 40/200, Loss: 0.0010\n",
            "Epoch 60/200, Loss: 0.0004\n",
            "Epoch 80/200, Loss: 0.0002\n",
            "Epoch 100/200, Loss: 0.0001\n",
            "Epoch 120/200, Loss: 0.0001\n",
            "Epoch 140/200, Loss: 0.0000\n",
            "Epoch 160/200, Loss: 0.0000\n",
            "Epoch 180/200, Loss: 0.0000\n",
            "Epoch 200/200, Loss: 0.0000\n",
            "--- Training Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Model Predictions and Attention ---\")\n",
        "# Use torch.no_grad() to turn off gradient calculations (we're just testing)\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Loop over our data one more time\n",
        "    for sentence, target_index, label in data:\n",
        "        print(f\"\\n---------------------------------\")\n",
        "        print(f\"Sentence: '{sentence}'\")\n",
        "\n",
        "        # Get the original words\n",
        "        words = sentence.lower().split()\n",
        "\n",
        "        # Prepare inputs\n",
        "        input_indices = sentence_to_tensor(sentence, vocab)\n",
        "\n",
        "        # Run the model\n",
        "        scores, attention_weights = model(input_indices, target_index)\n",
        "\n",
        "        # Get the prediction\n",
        "        predicted_class = torch.argmax(scores).item()\n",
        "        predicted_meaning = \"Financial\" if predicted_class == 0 else \"River\"\n",
        "        actual_meaning = \"Financial\" if label == 0 else \"River\"\n",
        "\n",
        "        print(f\"  Target Word: '{words[target_index]}'\")\n",
        "        print(f\"  Prediction:  {predicted_meaning} (Actual: {actual_meaning})\")\n",
        "\n",
        "        # --- This is the key part ---\n",
        "        # The 'attention_weights' matrix is [seq_len, seq_len]\n",
        "        # We want the weights for our *target word*\n",
        "        # This row shows how much the target word \"paid attention\" to every other word\n",
        "\n",
        "        target_word_attention = attention_weights[target_index]\n",
        "\n",
        "        print(f\"  Attention weights for '{words[target_index]}':\")\n",
        "        for word, weight in zip(words, target_word_attention):\n",
        "            # Print the word and its corresponding attention score\n",
        "            print(f\"    {word:<10} : {weight.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY6IyGENoKC5",
        "outputId": "cf222025-3372-4daf-8c78-fc6c9419872d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Predictions and Attention ---\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'I went to the bank to deposit money'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    i          : 0.0000\n",
            "    went       : 0.0000\n",
            "    to         : 0.0001\n",
            "    the        : 0.0042\n",
            "    bank       : 0.0000\n",
            "    to         : 0.0001\n",
            "    deposit    : 0.9952\n",
            "    money      : 0.0003\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'He sat on the river bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    he         : 0.0399\n",
            "    sat        : 0.0000\n",
            "    on         : 0.0247\n",
            "    the        : 0.0004\n",
            "    river      : 0.9349\n",
            "    bank       : 0.0000\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The savings bank is closed today'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0050\n",
            "    savings    : 0.9825\n",
            "    bank       : 0.0000\n",
            "    is         : 0.0005\n",
            "    closed     : 0.0001\n",
            "    today      : 0.0119\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The boat is near the bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.4604\n",
            "    boat       : 0.0018\n",
            "    is         : 0.0483\n",
            "    near       : 0.0256\n",
            "    the        : 0.4604\n",
            "    bank       : 0.0035\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'I need a loan from the bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    i          : 0.0000\n",
            "    need       : 0.9991\n",
            "    a          : 0.0006\n",
            "    loan       : 0.0002\n",
            "    from       : 0.0001\n",
            "    the        : 0.0001\n",
            "    bank       : 0.0000\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The river bank was muddy'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0004\n",
            "    river      : 0.9924\n",
            "    bank       : 0.0000\n",
            "    was        : 0.0003\n",
            "    muddy      : 0.0069\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The river bank is high today'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0004\n",
            "    river      : 0.9898\n",
            "    bank       : 0.0000\n",
            "    is         : 0.0000\n",
            "    high       : 0.0087\n",
            "    today      : 0.0010\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The bank is open for business'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0137\n",
            "    bank       : 0.0001\n",
            "    is         : 0.0014\n",
            "    open       : 0.0000\n",
            "    for        : 0.0010\n",
            "    business   : 0.9837\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'This bank is my favorite for loans'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    this       : 0.0000\n",
            "    bank       : 0.0000\n",
            "    is         : 0.0000\n",
            "    my         : 1.0000\n",
            "    favorite   : 0.0000\n",
            "    for        : 0.0000\n",
            "    loans      : 0.0000\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'We went to the river bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    we         : 0.0000\n",
            "    went       : 0.0000\n",
            "    to         : 0.0000\n",
            "    the        : 0.0004\n",
            "    river      : 0.9995\n",
            "    bank       : 0.0000\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'He opened a new bank account'\n",
            "  Target Word: 'new'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'new':\n",
            "    he         : 0.0150\n",
            "    opened     : 0.0219\n",
            "    a          : 0.0135\n",
            "    new        : 0.9144\n",
            "    bank       : 0.0182\n",
            "    account    : 0.0170\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'I need to transfer money at the bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    i          : 0.0000\n",
            "    need       : 0.9973\n",
            "    to         : 0.0000\n",
            "    transfer   : 0.0025\n",
            "    money      : 0.0000\n",
            "    at         : 0.0001\n",
            "    the        : 0.0001\n",
            "    bank       : 0.0000\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The bank approved my mortgage'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0000\n",
            "    bank       : 0.0000\n",
            "    approved   : 0.0000\n",
            "    my         : 1.0000\n",
            "    mortgage   : 0.0000\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'My paycheck goes to the bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    my         : 1.0000\n",
            "    paycheck   : 0.0000\n",
            "    goes       : 0.0000\n",
            "    to         : 0.0000\n",
            "    the        : 0.0000\n",
            "    bank       : 0.0000\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The bank charged an overdraft fee'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0001\n",
            "    bank       : 0.0000\n",
            "    charged    : 0.0001\n",
            "    an         : 0.9947\n",
            "    overdraft  : 0.0050\n",
            "    fee        : 0.0000\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'Online bank security is important'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    online     : 0.0000\n",
            "    bank       : 0.0000\n",
            "    security   : 0.0053\n",
            "    is         : 0.0000\n",
            "    important  : 0.9947\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The ATM at the bank is broken'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0028\n",
            "    atm        : 0.0000\n",
            "    at         : 0.0012\n",
            "    the        : 0.0028\n",
            "    bank       : 0.0000\n",
            "    is         : 0.0003\n",
            "    broken     : 0.9929\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'He works at the bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  Financial (Actual: Financial)\n",
            "  Attention weights for 'bank':\n",
            "    he         : 0.9853\n",
            "    works      : 0.0000\n",
            "    at         : 0.0044\n",
            "    the        : 0.0102\n",
            "    bank       : 0.0001\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'We had a picnic on the river bank'\n",
            "  Target Word: 'river'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'river':\n",
            "    we         : 0.8044\n",
            "    had        : 0.0200\n",
            "    a          : 0.0073\n",
            "    picnic     : 0.0056\n",
            "    on         : 0.0180\n",
            "    the        : 0.0807\n",
            "    river      : 0.0571\n",
            "    bank       : 0.0071\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The fishing boat passed the bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0000\n",
            "    fishing    : 0.9935\n",
            "    boat       : 0.0000\n",
            "    passed     : 0.0064\n",
            "    the        : 0.0000\n",
            "    bank       : 0.0000\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'Erosion is affecting the bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    erosion    : 0.0218\n",
            "    is         : 0.0012\n",
            "    affecting  : 0.9656\n",
            "    the        : 0.0113\n",
            "    bank       : 0.0001\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'We saw ducks on the bank'\n",
            "  Target Word: 'the'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'the':\n",
            "    we         : 0.2692\n",
            "    saw        : 0.6875\n",
            "    ducks      : 0.0019\n",
            "    on         : 0.0014\n",
            "    the        : 0.0099\n",
            "    bank       : 0.0300\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The bank was steep and grassy'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0700\n",
            "    bank       : 0.0005\n",
            "    was        : 0.0440\n",
            "    steep      : 0.8812\n",
            "    and        : 0.0004\n",
            "    grassy     : 0.0039\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'Flood waters rose over the bank'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    flood      : 0.0223\n",
            "    waters     : 0.0365\n",
            "    rose       : 0.0794\n",
            "    over       : 0.0159\n",
            "    the        : 0.8395\n",
            "    bank       : 0.0064\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'The north bank of the river is flooded'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    the        : 0.0004\n",
            "    north      : 0.0046\n",
            "    bank       : 0.0000\n",
            "    of         : 0.0000\n",
            "    the        : 0.0004\n",
            "    river      : 0.9930\n",
            "    is         : 0.0000\n",
            "    flooded    : 0.0015\n",
            "\n",
            "---------------------------------\n",
            "Sentence: 'Trees line the bank of the stream'\n",
            "  Target Word: 'bank'\n",
            "  Prediction:  River (Actual: River)\n",
            "  Attention weights for 'bank':\n",
            "    trees      : 0.0022\n",
            "    line       : 0.0024\n",
            "    the        : 0.0101\n",
            "    bank       : 0.0001\n",
            "    of         : 0.0000\n",
            "    the        : 0.0101\n",
            "    stream     : 0.9752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4c527a0"
      },
      "source": [
        "# Simple Word Sense Disambiguation with Self-Attention\n",
        "\n",
        "This notebook demonstrates a basic implementation of a self-attention mechanism to perform word sense disambiguation. The goal is to determine the correct meaning of the word \"bank\" in different sentences, classifying it as either \"Financial\" or \"River\".\n",
        "\n",
        "## Process\n",
        "\n",
        "The notebook follows these steps:\n",
        "\n",
        "1.  **Data Preparation**: A small dataset is defined containing sentences with the word \"bank\", the index of the target word \"bank\" (or a word related to the meaning in some cases), and the corresponding label (Financial or River).\n",
        "\n",
        "2.  **Vocabulary Creation**: A vocabulary is built from all the unique words in the dataset, mapping each word to a unique integer ID. A special `<PAD>` token is included at index 0.\n",
        "\n",
        "3.  **Model Definition**: A simple self-attention model is defined using PyTorch.\n",
        "    *   An embedding layer converts word IDs into dense vectors.\n",
        "    *   Linear layers (`q_layer`, `k_layer`, `v_layer`) are used to create the Query, Key, and Value matrices for the self-attention calculation.\n",
        "    *   The attention scores are calculated by multiplying the Query and Key matrices, scaled by the square root of the embedding dimension.\n",
        "    *   A softmax function is applied to the scaled scores to get attention weights, indicating how much each word attends to every other word in the sentence.\n",
        "    *   The output of the attention mechanism is a weighted sum of the Value vectors.\n",
        "    *   A final linear layer (`output_layer`) takes the vector corresponding to the target word and predicts the class (Financial or River).\n",
        "\n",
        "4.  **Model Initialization**: The model is initialized with the vocabulary size, embedding dimension, and number of output classes. A loss function (CrossEntropyLoss) and an optimizer (Adam) are set up for training.\n",
        "\n",
        "5.  **Training**: The model is trained on the prepared dataset for a specified number of epochs. In each training step:\n",
        "    *   Gradients are zeroed.\n",
        "    *   The sentence and target word index are converted to tensors.\n",
        "    *   The model performs a forward pass to get predicted scores and attention weights.\n",
        "    *   The loss is calculated based on the predicted scores and the actual label.\n",
        "    *   The gradients are computed using the backward pass.\n",
        "    *   The model's weights are updated by the optimizer.\n",
        "\n",
        "6.  **Inference and Attention Visualization**: After training, the model's predictions are evaluated on the training data. For each sentence, the predicted meaning is shown alongside the actual meaning. Crucially, the attention weights for the target word are displayed, showing which words the model focused on when making its classification decision.\n",
        "\n",
        "This example highlights how a simple attention mechanism can learn to weigh the importance of different words in a sentence to understand the context and disambiguate word senses."
      ]
    }
  ]
}